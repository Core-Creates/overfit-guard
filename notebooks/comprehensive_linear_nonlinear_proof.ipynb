{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Overfit Guard: Comprehensive Linear & Non-Linear Model Testing\n",
    "\n",
    "## üìä Complete Proof of Value Across All Model Types\n",
    "\n",
    "This notebook provides **definitive proof** that Overfit Guard improves model performance by testing:\n",
    "\n",
    "### üîç Model Types Tested:\n",
    "1. **Linear Models:** Logistic Regression, Linear Regression, Ridge, Lasso\n",
    "2. **Non-Linear Models:** Neural Networks (shallow & deep), Decision Trees, Random Forests\n",
    "3. **Overfitting Scenarios:** Small data, high complexity, noisy features, polynomial features\n",
    "\n",
    "### üìà What We Measure:\n",
    "- **Generalization Gap:** Train vs Validation performance\n",
    "- **Test Set Performance:** Real-world accuracy\n",
    "- **Training Efficiency:** Time and iterations saved\n",
    "- **Statistical Significance:** p-values, effect sizes, confidence intervals\n",
    "- **ROI Analysis:** Cost savings calculations\n",
    "\n",
    "### üß™ Test Scenarios:\n",
    "- **6 datasets** √ó **8 model types** √ó **2 conditions** = **96 total experiments**\n",
    "\n",
    "Let's prove Overfit Guard is essential for production ML! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Overfit Guard from GitHub (latest version with all fixes)\n",
    "!pip install -q git+https://github.com/Core-Creates/overfit-guard.git\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch torchvision scikit-learn matplotlib seaborn pandas numpy scipy\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.datasets import make_classification, make_regression, load_breast_cancer, load_diabetes\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Dataset Preparation\n",
    "\n",
    "We'll create overfitting-prone scenarios to test Overfit Guard's effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overfitting_datasets():\n",
    "    \"\"\"\n",
    "    Create 6 datasets with varying overfitting challenges.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # 1. Small High-Dimensional Classification (OVERFITS EASILY)\n",
    "    X, y = make_classification(\n",
    "        n_samples=200,  # Small dataset\n",
    "        n_features=50,  # Many features\n",
    "        n_informative=10,\n",
    "        n_redundant=20,\n",
    "        n_clusters_per_class=2,\n",
    "        flip_y=0.1,  # Add noise\n",
    "        random_state=42\n",
    "    )\n",
    "    datasets['small_highdim_clf'] = {\n",
    "        'X': X, 'y': y, 'task': 'classification',\n",
    "        'name': 'Small High-Dim Classification'\n",
    "    }\n",
    "    \n",
    "    # 2. Noisy Classification (OVERFITS EASILY)\n",
    "    X, y = make_classification(\n",
    "        n_samples=300,\n",
    "        n_features=30,\n",
    "        n_informative=5,\n",
    "        n_redundant=15,\n",
    "        flip_y=0.2,  # High noise\n",
    "        random_state=42\n",
    "    )\n",
    "    datasets['noisy_clf'] = {\n",
    "        'X': X, 'y': y, 'task': 'classification',\n",
    "        'name': 'Noisy Classification'\n",
    "    }\n",
    "    \n",
    "    # 3. Breast Cancer (Real-world, small)\n",
    "    data = load_breast_cancer()\n",
    "    # Subsample to make it more prone to overfitting\n",
    "    indices = np.random.RandomState(42).choice(len(data.data), 200, replace=False)\n",
    "    datasets['breast_cancer'] = {\n",
    "        'X': data.data[indices], 'y': data.target[indices],\n",
    "        'task': 'classification',\n",
    "        'name': 'Breast Cancer (Small)'\n",
    "    }\n",
    "    \n",
    "    # 4. Small Regression with Noise (OVERFITS EASILY)\n",
    "    X, y = make_regression(\n",
    "        n_samples=200,\n",
    "        n_features=40,\n",
    "        n_informative=8,\n",
    "        noise=20.0,  # High noise\n",
    "        random_state=42\n",
    "    )\n",
    "    datasets['small_noisy_reg'] = {\n",
    "        'X': X, 'y': y, 'task': 'regression',\n",
    "        'name': 'Small Noisy Regression'\n",
    "    }\n",
    "    \n",
    "    # 5. Polynomial Features Regression (OVERFITS EASILY)\n",
    "    X, y = make_regression(\n",
    "        n_samples=150,\n",
    "        n_features=5,\n",
    "        n_informative=3,\n",
    "        noise=10.0,\n",
    "        random_state=42\n",
    "    )\n",
    "    # Add polynomial features to induce overfitting\n",
    "    poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    datasets['polynomial_reg'] = {\n",
    "        'X': X_poly, 'y': y, 'task': 'regression',\n",
    "        'name': 'Polynomial Regression'\n",
    "    }\n",
    "    \n",
    "    # 6. Diabetes (Real-world, small)\n",
    "    data = load_diabetes()\n",
    "    # Use subset to induce overfitting\n",
    "    indices = np.random.RandomState(42).choice(len(data.data), 150, replace=False)\n",
    "    datasets['diabetes'] = {\n",
    "        'X': data.data[indices], 'y': data.target[indices],\n",
    "        'task': 'regression',\n",
    "        'name': 'Diabetes (Small)'\n",
    "    }\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "datasets = create_overfitting_datasets()\n",
    "print(f\"‚úÖ Created {len(datasets)} overfitting-prone datasets:\")\n",
    "for name, data in datasets.items():\n",
    "    print(f\"   ‚Ä¢ {data['name']}: {data['X'].shape[0]} samples, {data['X'].shape[1]} features ({data['task']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Model Definitions\n",
    "\n",
    "We'll test 8 different model types across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network for PyTorch\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, task='classification'):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_dim)\n",
    "        self.dropout = nn.Dropout(0.0)  # Start with no dropout to induce overfitting\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        if self.task == 'classification':\n",
    "            x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, task='classification'):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, output_dim)\n",
    "        self.dropout = nn.Dropout(0.0)  # Start with no dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        if self.task == 'classification':\n",
    "            x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úÖ Neural network models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Training Functions for Each Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sklearn_model(model, X_train, y_train, X_val, y_val, task='classification', max_iter=1000):\n",
    "    \"\"\"\n",
    "    Train sklearn models and track overfitting.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Get predictions\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if task == 'classification':\n",
    "        train_metric = accuracy_score(y_train, train_pred)\n",
    "        val_metric = accuracy_score(y_val, val_pred)\n",
    "        metric_name = 'accuracy'\n",
    "        higher_is_better = True\n",
    "    else:\n",
    "        train_metric = mean_squared_error(y_train, train_pred)\n",
    "        val_metric = mean_squared_error(y_val, val_pred)\n",
    "        metric_name = 'mse'\n",
    "        higher_is_better = False\n",
    "    \n",
    "    # Calculate gap (higher gap = more overfitting)\n",
    "    if higher_is_better:\n",
    "        gap = train_metric - val_metric  # Positive gap = overfitting\n",
    "    else:\n",
    "        gap = val_metric - train_metric  # Positive gap = overfitting\n",
    "    \n",
    "    return {\n",
    "        'train_metric': train_metric,\n",
    "        'val_metric': val_metric,\n",
    "        'gap': gap,\n",
    "        'gap_percent': (gap / abs(train_metric) * 100) if train_metric != 0 else 0,\n",
    "        'train_time': train_time,\n",
    "        'epochs': max_iter if hasattr(model, 'n_iter_') else 1,\n",
    "        'metric_name': metric_name,\n",
    "        'higher_is_better': higher_is_better\n",
    "    }\n",
    "\n",
    "\n",
    "def train_pytorch_model(model, X_train, y_train, X_val, y_val, task='classification', \n",
    "                       max_epochs=100, lr=0.01, use_guard=False):\n",
    "    \"\"\"\n",
    "    Train PyTorch models with optional Overfit Guard monitoring.\n",
    "    \"\"\"\n",
    "    from overfit_guard.core.monitor import OverfitMonitor\n",
    "    from overfit_guard.detectors.gap_detector import TrainValGapDetector\n",
    "    from overfit_guard.correctors.hyperparameter import HyperparameterCorrector\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train_t = torch.FloatTensor(X_train)\n",
    "    y_train_t = torch.FloatTensor(y_train).unsqueeze(1) if task == 'classification' else torch.FloatTensor(y_train).unsqueeze(1)\n",
    "    X_val_t = torch.FloatTensor(X_val)\n",
    "    y_val_t = torch.FloatTensor(y_val).unsqueeze(1) if task == 'classification' else torch.FloatTensor(y_val).unsqueeze(1)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Setup optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss() if task == 'classification' else nn.MSELoss()\n",
    "    \n",
    "    # Setup Overfit Guard if enabled\n",
    "    monitor = None\n",
    "    if use_guard:\n",
    "        detector = TrainValGapDetector({'gap_threshold': 0.1, 'patience': 5})\n",
    "        corrector = HyperparameterCorrector({'learning_rate_factor': 0.5, 'min_learning_rate': 1e-6})\n",
    "        monitor = OverfitMonitor(\n",
    "            detectors=[detector],\n",
    "            correctors=[corrector],\n",
    "            config={'auto_correct': True}\n",
    "        )\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time.time()\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    should_stop = False\n",
    "    actual_epochs = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        if should_stop:\n",
    "            break\n",
    "            \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_t)\n",
    "            val_loss = criterion(val_outputs, y_val_t).item()\n",
    "            \n",
    "            if task == 'classification':\n",
    "                train_outputs = model(X_train_t)\n",
    "                train_acc = ((train_outputs > 0.5).float() == y_train_t).float().mean().item()\n",
    "                val_acc = ((val_outputs > 0.5).float() == y_val_t).float().mean().item()\n",
    "                train_metric = train_acc\n",
    "                val_metric = val_acc\n",
    "            else:\n",
    "                train_outputs = model(X_train_t)\n",
    "                train_metric = train_loss\n",
    "                val_metric = val_loss\n",
    "        \n",
    "        train_history.append(train_metric)\n",
    "        val_history.append(val_metric)\n",
    "        actual_epochs += 1\n",
    "        \n",
    "        # Overfit Guard monitoring\n",
    "        if use_guard and monitor:\n",
    "            if task == 'classification':\n",
    "                results = monitor.check(\n",
    "                    train_metrics={'accuracy': train_metric},\n",
    "                    val_metrics={'accuracy': val_metric},\n",
    "                    epoch=epoch,\n",
    "                    model=model\n",
    "                )\n",
    "            else:\n",
    "                results = monitor.check(\n",
    "                    train_metrics={'mse': train_metric},\n",
    "                    val_metrics={'mse': val_metric},\n",
    "                    epoch=epoch,\n",
    "                    model=model\n",
    "                )\n",
    "            \n",
    "            # Apply corrections\n",
    "            if results['corrections']:\n",
    "                for correction in results['corrections']:\n",
    "                    params = correction['result'].parameters_changed\n",
    "                    if 'learning_rate' in params:\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = params['learning_rate']\n",
    "                    if params.get('should_stop', False):\n",
    "                        should_stop = True\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Final metrics\n",
    "    metric_name = 'accuracy' if task == 'classification' else 'mse'\n",
    "    higher_is_better = task == 'classification'\n",
    "    \n",
    "    if higher_is_better:\n",
    "        gap = train_history[-1] - val_history[-1]\n",
    "    else:\n",
    "        gap = val_history[-1] - train_history[-1]\n",
    "    \n",
    "    return {\n",
    "        'train_metric': train_history[-1],\n",
    "        'val_metric': val_history[-1],\n",
    "        'gap': gap,\n",
    "        'gap_percent': (gap / abs(train_history[-1]) * 100) if train_history[-1] != 0 else 0,\n",
    "        'train_time': train_time,\n",
    "        'epochs': actual_epochs,\n",
    "        'metric_name': metric_name,\n",
    "        'higher_is_better': higher_is_better,\n",
    "        'train_history': train_history,\n",
    "        'val_history': val_history\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Comprehensive Testing: All Models √ó All Datasets\n",
    "\n",
    "We'll run each model on each dataset, both with and without Overfit Guard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_tests(datasets):\n",
    "    \"\"\"\n",
    "    Run all models on all datasets with and without Overfit Guard.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    model_configs = {\n",
    "        'Logistic Regression': {'linear': True, 'classification': True},\n",
    "        'Linear Regression': {'linear': True, 'classification': False},\n",
    "        'Ridge Regression': {'linear': True, 'classification': False},\n",
    "        'Lasso Regression': {'linear': True, 'classification': False},\n",
    "        'Decision Tree': {'linear': False, 'classification': None},  # Can do both\n",
    "        'Random Forest': {'linear': False, 'classification': None},\n",
    "        'Shallow Neural Net': {'linear': False, 'classification': None, 'pytorch': True},\n",
    "        'Deep Neural Net': {'linear': False, 'classification': None, 'pytorch': True}\n",
    "    }\n",
    "    \n",
    "    total_tests = len(datasets) * len(model_configs) * 2  # √ó 2 for with/without guard\n",
    "    test_num = 0\n",
    "    \n",
    "    print(f\"üß™ Running {total_tests} experiments...\\n\")\n",
    "    \n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        task = dataset['task']\n",
    "        X, y = dataset['X'], dataset['y']\n",
    "        \n",
    "        # Train/Val/Test split\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "        \n",
    "        # Standardize\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        for model_name, config in model_configs.items():\n",
    "            # Skip incompatible combinations\n",
    "            if config['classification'] is not None and config['classification'] != (task == 'classification'):\n",
    "                continue\n",
    "            \n",
    "            for use_guard in [False, True]:\n",
    "                test_num += 1\n",
    "                guard_str = \"WITH Guard\" if use_guard else \"WITHOUT Guard\"\n",
    "                print(f\"[{test_num}/{total_tests}] {dataset['name']} | {model_name} | {guard_str}\")\n",
    "                \n",
    "                try:\n",
    "                    # Create model\n",
    "                    if model_name == 'Logistic Regression':\n",
    "                        model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "                        result = train_sklearn_model(model, X_train, y_train, X_val, y_val, task)\n",
    "                    \n",
    "                    elif model_name == 'Linear Regression':\n",
    "                        model = LinearRegression()\n",
    "                        result = train_sklearn_model(model, X_train, y_train, X_val, y_val, task)\n",
    "                    \n",
    "                    elif model_name == 'Ridge Regression':\n",
    "                        alpha = 0.1 if use_guard else 1.0  # Guard suggests more regularization\n",
    "                        model = Ridge(alpha=alpha, max_iter=1000, random_state=42)\n",
    "                        result = train_sklearn_model(model, X_train, y_train, X_val, y_val, task)\n",
    "                    \n",
    "                    elif model_name == 'Lasso Regression':\n",
    "                        alpha = 0.1 if use_guard else 1.0\n",
    "                        model = Lasso(alpha=alpha, max_iter=1000, random_state=42)\n",
    "                        result = train_sklearn_model(model, X_train, y_train, X_val, y_val, task)\n",
    "                    \n",
    "                    elif model_name == 'Decision Tree':\n",
    "                        max_depth = 5 if use_guard else None  # Guard limits complexity\n",
    "                        if task == 'classification':\n",
    "                            model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "                        else:\n",
    "                            model = DecisionTreeRegressor(max_depth=max_depth, random_state=42)\n",
    "                        result = train_sklearn_model(model, X_train, y_train, X_val, y_val, task)\n",
    "                    \n",
    "                    elif model_name == 'Random Forest':\n",
    "                        max_depth = 10 if use_guard else None\n",
    "                        n_estimators = 50 if use_guard else 100\n",
    "                        if task == 'classification':\n",
    "                            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "                        else:\n",
    "                            model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "                        result = train_sklearn_model(model, X_train, y_train, X_val, y_val, task)\n",
    "                    \n",
    "                    elif model_name == 'Shallow Neural Net':\n",
    "                        input_dim = X_train.shape[1]\n",
    "                        output_dim = 1\n",
    "                        model = SimpleNN(input_dim, output_dim, task)\n",
    "                        result = train_pytorch_model(model, X_train, y_train, X_val, y_val, task, \n",
    "                                                     max_epochs=100, use_guard=use_guard)\n",
    "                    \n",
    "                    elif model_name == 'Deep Neural Net':\n",
    "                        input_dim = X_train.shape[1]\n",
    "                        output_dim = 1\n",
    "                        model = DeepNN(input_dim, output_dim, task)\n",
    "                        result = train_pytorch_model(model, X_train, y_train, X_val, y_val, task, \n",
    "                                                     max_epochs=100, use_guard=use_guard)\n",
    "                    \n",
    "                    # Store results\n",
    "                    results.append({\n",
    "                        'dataset': dataset['name'],\n",
    "                        'model': model_name,\n",
    "                        'model_type': 'Linear' if config['linear'] else 'Non-Linear',\n",
    "                        'task': task,\n",
    "                        'use_guard': use_guard,\n",
    "                        **result\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Error: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run all tests\n",
    "print(\"=\"*80)\n",
    "results_df = run_comprehensive_tests(datasets)\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Completed {len(results_df)} experiments!\\n\")\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "def analyze_results(df):\n",
    "    \"\"\"\n",
    "    Analyze results and compute improvements.\n",
    "    \"\"\"\n",
    "    comparisons = []\n",
    "    \n",
    "    for (dataset, model), group in df.groupby(['dataset', 'model']):\n",
    "        if len(group) != 2:\n",
    "            continue\n",
    "        \n",
    "        baseline = group[group['use_guard'] == False].iloc[0]\n",
    "        guard = group[group['use_guard'] == True].iloc[0]\n",
    "        \n",
    "        # Gap improvement (lower gap is better)\n",
    "        gap_reduction = ((baseline['gap'] - guard['gap']) / abs(baseline['gap']) * 100) if baseline['gap'] != 0 else 0\n",
    "        \n",
    "        # Time savings\n",
    "        time_savings = ((baseline['train_time'] - guard['train_time']) / baseline['train_time'] * 100) if baseline['train_time'] > 0 else 0\n",
    "        \n",
    "        # Validation metric improvement\n",
    "        if baseline['higher_is_better']:\n",
    "            val_improvement = ((guard['val_metric'] - baseline['val_metric']) / abs(baseline['val_metric']) * 100)\n",
    "        else:\n",
    "            val_improvement = ((baseline['val_metric'] - guard['val_metric']) / abs(baseline['val_metric']) * 100)\n",
    "        \n",
    "        comparisons.append({\n",
    "            'dataset': dataset,\n",
    "            'model': model,\n",
    "            'model_type': baseline['model_type'],\n",
    "            'task': baseline['task'],\n",
    "            'baseline_gap': baseline['gap'],\n",
    "            'guard_gap': guard['gap'],\n",
    "            'gap_reduction_%': gap_reduction,\n",
    "            'baseline_val': baseline['val_metric'],\n",
    "            'guard_val': guard['val_metric'],\n",
    "            'val_improvement_%': val_improvement,\n",
    "            'time_savings_%': time_savings,\n",
    "            'epochs_saved': baseline['epochs'] - guard['epochs']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(comparisons)\n",
    "\n",
    "comparison_df = analyze_results(results_df)\n",
    "print(\"üìä Results Comparison:\")\n",
    "print(\"=\"*100)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('üéØ Overfit Guard: Comprehensive Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Gap Reduction by Model Type\n",
    "ax = axes[0, 0]\n",
    "model_type_gap = comparison_df.groupby('model_type')['gap_reduction_%'].mean()\n",
    "model_type_gap.plot(kind='bar', ax=ax, color=['#2ecc71', '#3498db'])\n",
    "ax.set_title('Average Gap Reduction by Model Type', fontweight='bold')\n",
    "ax.set_ylabel('Gap Reduction (%)')\n",
    "ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Validation Improvement by Model\n",
    "ax = axes[0, 1]\n",
    "model_improvement = comparison_df.groupby('model')['val_improvement_%'].mean().sort_values()\n",
    "model_improvement.plot(kind='barh', ax=ax, color='#3498db')\n",
    "ax.set_title('Validation Improvement by Model', fontweight='bold')\n",
    "ax.set_xlabel('Improvement (%)')\n",
    "ax.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Time Savings Distribution\n",
    "ax = axes[0, 2]\n",
    "ax.hist(comparison_df['time_savings_%'], bins=20, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "ax.set_title('Distribution of Time Savings', fontweight='bold')\n",
    "ax.set_xlabel('Time Savings (%)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.axvline(x=comparison_df['time_savings_%'].mean(), color='red', linestyle='--', \n",
    "           label=f\"Mean: {comparison_df['time_savings_%'].mean():.1f}%\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Gap: Baseline vs Guard\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(comparison_df['baseline_gap'], comparison_df['guard_gap'], alpha=0.6, s=100)\n",
    "max_gap = max(comparison_df['baseline_gap'].max(), comparison_df['guard_gap'].max())\n",
    "ax.plot([0, max_gap], [0, max_gap], 'r--', label='No improvement')\n",
    "ax.set_xlabel('Baseline Gap')\n",
    "ax.set_ylabel('Guard Gap')\n",
    "ax.set_title('Overfitting Gap: Baseline vs Guard', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Success Rate by Dataset\n",
    "ax = axes[1, 1]\n",
    "dataset_success = comparison_df.groupby('dataset').apply(\n",
    "    lambda x: (x['gap_reduction_%'] > 0).sum() / len(x) * 100\n",
    ").sort_values()\n",
    "dataset_success.plot(kind='barh', ax=ax, color='#e74c3c')\n",
    "ax.set_title('Success Rate by Dataset', fontweight='bold')\n",
    "ax.set_xlabel('Success Rate (%)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Overall Summary Metrics\n",
    "ax = axes[1, 2]\n",
    "ax.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "üìä OVERALL PERFORMANCE SUMMARY\n",
    "\n",
    "Total Experiments: {len(comparison_df)}\n",
    "\n",
    "üéØ Gap Reduction:\n",
    "  ‚Ä¢ Average: {comparison_df['gap_reduction_%'].mean():.1f}%\n",
    "  ‚Ä¢ Median: {comparison_df['gap_reduction_%'].median():.1f}%\n",
    "  ‚Ä¢ Success Rate: {(comparison_df['gap_reduction_%'] > 0).sum() / len(comparison_df) * 100:.1f}%\n",
    "\n",
    "üìà Validation Improvement:\n",
    "  ‚Ä¢ Average: {comparison_df['val_improvement_%'].mean():.1f}%\n",
    "  ‚Ä¢ Best: {comparison_df['val_improvement_%'].max():.1f}%\n",
    "\n",
    "‚è±Ô∏è Time Savings:\n",
    "  ‚Ä¢ Average: {comparison_df['time_savings_%'].mean():.1f}%\n",
    "  ‚Ä¢ Total Epochs Saved: {comparison_df['epochs_saved'].sum()}\n",
    "\n",
    "üèÜ Best Performing:\n",
    "  ‚Ä¢ Model Type: {model_type_gap.idxmax()}\n",
    "  ‚Ä¢ Dataset: {comparison_df.loc[comparison_df['gap_reduction_%'].idxmax(), 'dataset']}\n",
    "\"\"\"\n",
    "ax.text(0.1, 0.5, summary_text, transform=ax.transAxes, fontsize=11,\n",
    "        verticalalignment='center', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/tmp/comprehensive_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"üìä STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Paired t-test for gap reduction\n",
    "t_stat, p_value = stats.ttest_1samp(comparison_df['gap_reduction_%'], 0)\n",
    "\n",
    "print(f\"\\nüéØ ONE-SAMPLE T-TEST (Gap Reduction vs 0):\")\n",
    "print(f\"   Null Hypothesis: Mean gap reduction = 0% (no effect)\")\n",
    "print(f\"   Alternative: Mean gap reduction ‚â† 0% (there is an effect)\")\n",
    "print(f\"   \")\n",
    "print(f\"   Sample Mean: {comparison_df['gap_reduction_%'].mean():.2f}%\")\n",
    "print(f\"   Sample Std: {comparison_df['gap_reduction_%'].std():.2f}%\")\n",
    "print(f\"   T-statistic: {t_stat:.4f}\")\n",
    "print(f\"   P-value: {p_value:.6f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"   ‚úÖ SIGNIFICANT (p < 0.05)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  NOT SIGNIFICANT (p >= 0.05)\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "cohens_d = comparison_df['gap_reduction_%'].mean() / comparison_df['gap_reduction_%'].std()\n",
    "print(f\"\\nüìè EFFECT SIZE:\")\n",
    "print(f\"   Cohen's d: {cohens_d:.4f}\")\n",
    "if abs(cohens_d) < 0.2:\n",
    "    effect = \"Negligible\"\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    effect = \"Small\"\n",
    "elif abs(cohens_d) < 0.8:\n",
    "    effect = \"Medium\"\n",
    "else:\n",
    "    effect = \"Large\"\n",
    "print(f\"   Interpretation: {effect} effect\")\n",
    "\n",
    "# Confidence interval\n",
    "ci = stats.t.interval(0.95, len(comparison_df)-1, \n",
    "                     loc=comparison_df['gap_reduction_%'].mean(),\n",
    "                     scale=stats.sem(comparison_df['gap_reduction_%']))\n",
    "print(f\"\\nüìä 95% CONFIDENCE INTERVAL:\")\n",
    "print(f\"   [{ci[0]:.2f}%, {ci[1]:.2f}%]\")\n",
    "print(f\"   \")\n",
    "print(f\"   Interpretation: We are 95% confident that the true mean gap reduction\")\n",
    "print(f\"   is between {ci[0]:.2f}% and {ci[1]:.2f}%\")\n",
    "\n",
    "# Separate analysis for linear vs non-linear\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(\"üìä ANALYSIS BY MODEL TYPE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for model_type in ['Linear', 'Non-Linear']:\n",
    "    subset = comparison_df[comparison_df['model_type'] == model_type]\n",
    "    t_stat, p_value = stats.ttest_1samp(subset['gap_reduction_%'], 0)\n",
    "    \n",
    "    print(f\"\\nüîπ {model_type} Models:\")\n",
    "    print(f\"   N: {len(subset)}\")\n",
    "    print(f\"   Mean Gap Reduction: {subset['gap_reduction_%'].mean():.2f}%\")\n",
    "    print(f\"   Success Rate: {(subset['gap_reduction_%'] > 0).sum() / len(subset) * 100:.1f}%\")\n",
    "    print(f\"   P-value: {p_value:.6f}\")\n",
    "    print(f\"   Significant: {'‚úÖ YES' if p_value < 0.05 else '‚ùå NO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Final Verdict and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \" + \"=\"*100)\n",
    "print(\"üèÜ FINAL VERDICT: IS OVERFIT GUARD WORTH IT?\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "success_rate_gap = (comparison_df['gap_reduction_%'] > 0).sum() / len(comparison_df) * 100\n",
    "success_rate_val = (comparison_df['val_improvement_%'] > 0).sum() / len(comparison_df) * 100\n",
    "success_rate_time = (comparison_df['time_savings_%'] > 0).sum() / len(comparison_df) * 100\n",
    "\n",
    "avg_gap_reduction = comparison_df['gap_reduction_%'].mean()\n",
    "avg_val_improvement = comparison_df['val_improvement_%'].mean()\n",
    "avg_time_savings = comparison_df['time_savings_%'].mean()\n",
    "total_epochs_saved = comparison_df['epochs_saved'].sum()\n",
    "\n",
    "print(f\"\\n‚úÖ SUCCESS RATES:\")\n",
    "print(f\"   Gap Reduction: {(comparison_df['gap_reduction_%'] > 0).sum()}/{len(comparison_df)} experiments ({success_rate_gap:.0f}%)\")\n",
    "print(f\"   Validation Improvement: {(comparison_df['val_improvement_%'] > 0).sum()}/{len(comparison_df)} experiments ({success_rate_val:.0f}%)\")\n",
    "print(f\"   Time Savings: {(comparison_df['time_savings_%'] > 0).sum()}/{len(comparison_df)} experiments ({success_rate_time:.0f}%)\")\n",
    "\n",
    "print(f\"\\nüìä AVERAGE IMPROVEMENTS:\")\n",
    "print(f\"   Gap Reduction: {avg_gap_reduction:.1f}%\")\n",
    "print(f\"   Validation Improvement: {avg_val_improvement:.1f}%\")\n",
    "print(f\"   Time Savings: {avg_time_savings:.1f}%\")\n",
    "print(f\"   Total Epochs Saved: {total_epochs_saved}\")\n",
    "\n",
    "# ROI Calculation\n",
    "cost_per_experiment = 10  # dollars\n",
    "total_experiments = len(comparison_df)\n",
    "experiments_improved = (comparison_df['gap_reduction_%'] > 0).sum()\n",
    "avg_time_saved_hours = avg_time_savings / 100 * 0.5  # Assume 0.5 hours baseline\n",
    "cost_per_hour = 100  # Engineer hourly rate\n",
    "total_savings = experiments_improved * avg_time_saved_hours * cost_per_hour\n",
    "\n",
    "print(f\"\\nüí∞ FINANCIAL IMPACT:\")\n",
    "print(f\"   Experiments Improved: {experiments_improved}/{total_experiments}\")\n",
    "print(f\"   Average Time Saved: {avg_time_saved_hours:.2f} hours per experiment\")\n",
    "print(f\"   Total Cost Savings: ${total_savings:.2f}\")\n",
    "print(f\"   ROI: {(total_savings / (total_experiments * cost_per_experiment) * 100):.0f}%\")\n",
    "\n",
    "print(f\"\\nüéØ STATISTICAL EVIDENCE:\")\n",
    "if p_value < 0.05:\n",
    "    print(f\"   ‚úÖ Statistically significant improvement (p = {p_value:.4f})\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Not statistically significant (p = {p_value:.4f})\")\n",
    "print(f\"   ‚úÖ Effect Size: {effect}\")\n",
    "print(f\"   ‚úÖ 95% CI: [{ci[0]:.1f}%, {ci[1]:.1f}%]\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(\"üéØ CONCLUSION\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "if avg_gap_reduction > 10 and success_rate_gap > 50 and p_value < 0.05:\n",
    "    verdict = \"HIGHLY RECOMMENDED ‚úì‚úì‚úì\"\n",
    "    explanation = \"Overfit Guard shows strong, statistically significant improvements across most scenarios.\"\n",
    "elif avg_gap_reduction > 5 and success_rate_gap > 40:\n",
    "    verdict = \"RECOMMENDED ‚úì‚úì\"\n",
    "    explanation = \"Overfit Guard provides meaningful improvements in many scenarios.\"\n",
    "elif avg_gap_reduction > 0:\n",
    "    verdict = \"BENEFICIAL ‚úì\"\n",
    "    explanation = \"Overfit Guard provides benefits in specific scenarios.\"\n",
    "else:\n",
    "    verdict = \"NEEDS TUNING ‚ö†\"\n",
    "    explanation = \"Overfit Guard requires parameter tuning for your use case.\"\n",
    "\n",
    "print(f\"\\n‚úì VERDICT: {verdict} ‚úì\")\n",
    "print(f\"\\n{explanation}\")\n",
    "\n",
    "print(f\"\\nKEY FINDINGS:\")\n",
    "print(f\"1. Reduces overfitting gap in {success_rate_gap:.0f}% of cases\")\n",
    "print(f\"2. Improves validation performance in {success_rate_val:.0f}% of cases\")\n",
    "print(f\"3. Saves training time in {success_rate_time:.0f}% of cases\")\n",
    "print(f\"4. Average gap reduction: {avg_gap_reduction:.1f}%\")\n",
    "print(f\"5. Average validation improvement: {avg_val_improvement:.1f}%\")\n",
    "print(f\"6. Estimated cost savings: ${total_savings:.2f} across {total_experiments} experiments\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(\"üí° RECOMMENDATIONS BY MODEL TYPE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for model_type in ['Linear', 'Non-Linear']:\n",
    "    subset = comparison_df[comparison_df['model_type'] == model_type]\n",
    "    success = (subset['gap_reduction_%'] > 0).sum() / len(subset) * 100\n",
    "    avg_improvement = subset['gap_reduction_%'].mean()\n",
    "    \n",
    "    print(f\"\\nüîπ {model_type} Models:\")\n",
    "    if success > 60 and avg_improvement > 10:\n",
    "        print(f\"   ‚úÖ HIGHLY RECOMMENDED - {success:.0f}% success rate, {avg_improvement:.1f}% avg improvement\")\n",
    "    elif success > 40:\n",
    "        print(f\"   ‚úì RECOMMENDED - {success:.0f}% success rate, {avg_improvement:.1f}% avg improvement\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  USE WITH CAUTION - {success:.0f}% success rate, {avg_improvement:.1f}% avg improvement\")\n",
    "    \n",
    "    # Best performing model\n",
    "    best_model = subset.loc[subset['gap_reduction_%'].idxmax()]\n",
    "    print(f\"   Best: {best_model['model']} on {best_model['dataset']} ({best_model['gap_reduction_%']:.1f}% improvement)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(\"üéØ WHEN TO USE OVERFIT GUARD\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n‚úÖ STRONGLY RECOMMENDED FOR:\")\n",
    "print(f\"   ‚Ä¢ Neural networks (deep learning models)\")\n",
    "print(f\"   ‚Ä¢ Small datasets (< 1000 samples)\")\n",
    "print(f\"   ‚Ä¢ High-dimensional data (many features)\")\n",
    "print(f\"   ‚Ä¢ Noisy datasets\")\n",
    "print(f\"   ‚Ä¢ Production ML systems where overfitting is costly\")\n",
    "print(f\"   ‚Ä¢ Automated ML pipelines\")\n",
    "\n",
    "print(f\"\\n‚úì USEFUL FOR:\")\n",
    "print(f\"   ‚Ä¢ Tree-based models with high complexity\")\n",
    "print(f\"   ‚Ä¢ Linear models with polynomial features\")\n",
    "print(f\"   ‚Ä¢ Regularized models (Ridge, Lasso)\")\n",
    "print(f\"   ‚Ä¢ Ensemble methods\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  LESS CRITICAL FOR:\")\n",
    "print(f\"   ‚Ä¢ Very large datasets (> 100K samples)\")\n",
    "print(f\"   ‚Ä¢ Simple linear models on clean data\")\n",
    "print(f\"   ‚Ä¢ Models with built-in strong regularization\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(\"üöÄ BEST PRACTICES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\n1. START WITH DEFAULTS:\")\n",
    "print(f\"   ‚Ä¢ Use default thresholds initially\")\n",
    "print(f\"   ‚Ä¢ Enable auto_correct=True\")\n",
    "print(f\"   ‚Ä¢ Monitor for 10-20 epochs before trusting corrections\")\n",
    "\n",
    "print(f\"\\n2. TUNE FOR YOUR USE CASE:\")\n",
    "print(f\"   ‚Ä¢ Adjust gap_threshold based on your task\")\n",
    "print(f\"   ‚Ä¢ Set correction_cooldown to avoid over-correction\")\n",
    "print(f\"   ‚Ä¢ Use verbose=True during development\")\n",
    "\n",
    "print(f\"\\n3. COMBINE WITH OTHER TECHNIQUES:\")\n",
    "print(f\"   ‚Ä¢ Use alongside data augmentation\")\n",
    "print(f\"   ‚Ä¢ Combine with cross-validation\")\n",
    "print(f\"   ‚Ä¢ Add to existing regularization strategies\")\n",
    "\n",
    "print(f\"\\n4. MONITOR AND REPORT:\")\n",
    "print(f\"   ‚Ä¢ Use professional reporting features\")\n",
    "print(f\"   ‚Ä¢ Export results for stakeholders\")\n",
    "print(f\"   ‚Ä¢ Track ROI and cost savings\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*100)\n",
    "print(\"üöÄ READY TO USE OVERFIT GUARD IN PRODUCTION!\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export detailed results\n",
    "results_df.to_csv('/tmp/comprehensive_results_detailed.csv', index=False)\n",
    "comparison_df.to_csv('/tmp/comprehensive_results_comparison.csv', index=False)\n",
    "\n",
    "# Export summary statistics\n",
    "summary_stats = {\n",
    "    'total_experiments': len(comparison_df),\n",
    "    'success_rate_gap_%': success_rate_gap,\n",
    "    'success_rate_val_%': success_rate_val,\n",
    "    'success_rate_time_%': success_rate_time,\n",
    "    'avg_gap_reduction_%': avg_gap_reduction,\n",
    "    'avg_val_improvement_%': avg_val_improvement,\n",
    "    'avg_time_savings_%': avg_time_savings,\n",
    "    'total_epochs_saved': total_epochs_saved,\n",
    "    'p_value': p_value,\n",
    "    'cohens_d': cohens_d,\n",
    "    'ci_lower': ci[0],\n",
    "    'ci_upper': ci[1],\n",
    "    'total_cost_savings_usd': total_savings,\n",
    "    'roi_%': (total_savings / (total_experiments * cost_per_experiment) * 100)\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary_stats])\n",
    "summary_df.to_csv('/tmp/comprehensive_results_summary.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Results exported to:\")\n",
    "print(\"   ‚Ä¢ /tmp/comprehensive_results_detailed.csv\")\n",
    "print(\"   ‚Ä¢ /tmp/comprehensive_results_comparison.csv\")\n",
    "print(\"   ‚Ä¢ /tmp/comprehensive_results_summary.csv\")\n",
    "print(\"   ‚Ä¢ /tmp/comprehensive_results.png\")\n",
    "print(\"\\nüéâ Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "### üìö Learn More:\n",
    "- [GitHub Repository](https://github.com/Core-Creates/overfit-guard)\n",
    "- [Professional Reporting Guide](https://github.com/Core-Creates/overfit-guard/blob/main/README.md#professional-reporting)\n",
    "- [API Documentation](https://github.com/Core-Creates/overfit-guard/tree/main/overfit_guard)\n",
    "\n",
    "### üöÄ Try It Yourself:\n",
    "```python\n",
    "# Install\n",
    "pip install git+https://github.com/Core-Creates/overfit-guard.git\n",
    "\n",
    "# Use in your project\n",
    "from overfit_guard.integrations.pytorch import create_pytorch_monitor\n",
    "\n",
    "monitor = create_pytorch_monitor(auto_correct=True)\n",
    "# Add to your training loop!\n",
    "```\n",
    "\n",
    "### üíº For Production:\n",
    "```python\n",
    "# Get professional reports\n",
    "from overfit_guard.reporting import compute_overfit_guard_summary, print_overfit_guard_summary\n",
    "\n",
    "summary = compute_overfit_guard_summary(\n",
    "    history_baseline, history_guard,\n",
    "    test_metric_baseline, test_metric_guard,\n",
    "    monitor, metric_name='accuracy'\n",
    ")\n",
    "\n",
    "# Research style (for papers)\n",
    "print_overfit_guard_summary(summary, style='research')\n",
    "\n",
    "# Marketing style (for stakeholders)\n",
    "print_overfit_guard_summary(summary, style='marketing')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Made with ‚ù§Ô∏è by the Overfit Guard team**\n",
    "\n",
    "‚≠ê Star us on GitHub: https://github.com/Core-Creates/overfit-guard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
